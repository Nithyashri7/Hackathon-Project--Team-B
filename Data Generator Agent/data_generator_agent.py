# -*- coding: utf-8 -*-
"""Data_Generator_Agent

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ovNXbnajc7gmtcXo4TMEHtXxVlgumXiY
"""

# --- Install Faker if not already installed ---
!pip install faker -q

# --- Import libraries ---
import pandas as pd
import numpy as np
from faker import Faker
from tqdm import tqdm

fake = Faker()

# --- Step 1: Define schema (you can modify this) ---
schema = {
    'name': {'type': 'string'},
    'age': {'type': 'int', 'min': 18, 'max': 70},
    'salary': {'type': 'float', 'min': 30000, 'max': 150000},
    'join_date': {'type': 'date'},
    'city': {'type': 'string'},
}

num_rows = 100000  # Number of rows to generate

# --- Step 2: Data generation function ---
def generate_data(schema, num_rows):
    data = {}
    for col, rules in tqdm(schema.items(), desc="Generating Columns"):
        dtype = rules['type']

        if dtype == 'int':
            data[col] = np.random.randint(rules['min'], rules['max'] + 1, num_rows)

        elif dtype == 'float':
            data[col] = np.round(np.random.uniform(rules['min'], rules['max'], num_rows), 2)

        elif dtype == 'string':
            if col.lower() == 'name':
                data[col] = [fake.first_name() for _ in range(num_rows)]
            elif col.lower() == 'city':
                data[col] = [fake.city() for _ in range(num_rows)]
            else:
                data[col] = [fake.word() for _ in range(num_rows)]

        elif dtype == 'date':
            data[col] = [fake.date_between(start_date='-5y', end_date='today') for _ in range(num_rows)]

        else:
            data[col] = ['N/A'] * num_rows
    return pd.DataFrame(data)

# --- Step 3: Generate dataset ---
print("ðŸš€ Generating synthetic dataset...")
df = generate_data(schema, num_rows)

# --- Step 4: Save to CSV ---
output_path = '/content/synthetic_dataset.csv'
df.to_csv(output_path, index=False)

print(f"\nâœ… Dataset generated successfully with {num_rows:,} rows.")
print(f"ðŸ“ File saved to: {output_path}")

# --- Step 5: Display sample ---
df.head()

import json
import random
import numpy as np

class SyntheticDataAgent:

    def __init__(self, schema_file):
        with open(schema_file, "r") as f:
            self.schema = json.load(f)

    def generate_value(self, col_name, info):
        if info["type"] == "categorical":
            return random.choice(info["top_values"])

        if info["type"] == "numeric":
            mean = info.get("mean", 1.0)
            std = info.get("std", 0.5)
            return round(np.random.normal(mean, std), 3)

        if info["type"] == "integer":
            low = info.get("min", 0)
            high = info.get("max", 10)
            return random.randint(low, high)

        return None

    def apply_relationships(self, row):
        """
        Example medical logic for breast cancer dataset
        Adjust rules for your dataset
        """

        # diagnosis depends on tumour measurements
        size_features = ["radius_mean", "area_mean", "perimeter_mean"]

        size_score = 0
        for f in size_features:
            if f in row:
                size_score += row[f]

        if size_score > 500:   # large tumour â†’ malignant
            row["diagnosis"] = "M"
        else:                  # small tumour â†’ benign
            row["diagnosis"] = "B"

        return row

    def generate_row(self):
        row = {}
        explanation = {}

        # generate raw values
        for col, info in self.schema.items():
            val = self.generate_value(col, info)
            row[col] = val
            explanation[col] = f"Generated {val} based on type={info['type']}"

        # apply relationships
        row = self.apply_relationships(row)

        # explain diagnosis logic
        explanation["diagnosis"] = (
            "Diagnosis assigned based on tumour size-related features. "
            "Large tumour values â†’ M, small values â†’ B."
        )

        return row, explanation

agent = SyntheticDataAgent("metadata.json")

row, explanation = agent.generate_row()

print("Generated Row:\n", row)
print("\nExplanation:\n", explanation)

# Colab-ready: Synthetic generator using only schema.json (Option C: rules + probabilistic)
!pip install faker tqdm -q

import json, math, os
import numpy as np
import pandas as pd
from faker import Faker
from tqdm import tqdm

# Colab files utility (only used if running in Colab)
try:
    from google.colab import files as colab_files
    _IN_COLAB = True
except Exception:
    _IN_COLAB = False

fake = Faker()
np.random.seed(42)

# -------------------------
# Helper: load schema.json
# -------------------------
# Place schema.json in current directory or upload in Colab
if _IN_COLAB:
    print("If running in Colab, please upload schema.json via the file picker.")
    uploaded = colab_files.upload()
    schema_file = None
    for k in uploaded.keys():
        if k.lower().endswith(".json"):
            schema_file = k
else:
    schema_file = "schema.json"
    if not os.path.exists(schema_file):
        raise FileNotFoundError("Please place schema.json in working directory (or run in Colab and upload).")

if schema_file is None:
    raise ValueError("No schema.json found or uploaded.")

with open(schema_file, "r") as f:
    schema = json.load(f)

# -------------------------
# Parse schema
# -------------------------
numeric_cols = []
categorical_cols = []
string_cols = []
date_cols = []
schema_rules = schema.get("rules", {})

for col, info in schema.items():
    t = info.get("type", "").lower()
    if t in ("numeric","number","float","double"):
        numeric_cols.append(col)
    elif t in ("int","integer"):
        numeric_cols.append(col)
    elif t in ("categorical","category"):
        categorical_cols.append(col)
    elif t in ("string","text"):
        string_cols.append(col)
    elif t in ("date","datetime"):
        date_cols.append(col)
    # ignore 'rules' key itself
# Remove possible 'rules' key if present
for reserved in ["rules"]:
    if reserved in numeric_cols: numeric_cols.remove(reserved)
    if reserved in categorical_cols: categorical_cols.remove(reserved)
    if reserved in string_cols: string_cols.remove(reserved)

print("Detected columns from schema:")
print(" Numeric:", numeric_cols)
print(" Categorical:", categorical_cols)
print(" String:", string_cols)
print(" Date:", date_cols)
print(" Extra rules:", bool(schema_rules))

# -------------------------
# Build numeric marginals using schema stats
# For each numeric column we will construct a distribution (truncated normal) using mean/std/min/max if present.
# If mean/std missing but min/max present, we'll use uniform or infer mean=(min+max)/2
# -------------------------
from scipy.stats import truncnorm

def make_numeric_sampler(info):
    # returns a function sampler(n) -> array
    mn = info.get("min", None)
    mx = info.get("max", None)
    mu = info.get("mean", None)
    sigma = info.get("std", None)

    if mu is None and (mn is not None and mx is not None):
        mu = 0.5*(mn+mx)
    if sigma is None:
        # rule-of-thumb assuming range covers ~6 sigma
        if mn is not None and mx is not None:
            sigma = max(1e-3, (mx-mn)/6)
        else:
            sigma = 1.0

    # if bounds missing, set wide bounds
    if mn is None:
        mn = mu - 4*sigma
    if mx is None:
        mx = mu + 4*sigma

    a = (mn - mu) / sigma
    b = (mx - mu) / sigma
    def sampler(n):
        if sigma <= 0:
            return np.full(n, mu)
        s = truncnorm.rvs(a, b, loc=mu, scale=sigma, size=n)
        return s
    return sampler, {"min":mn,"max":mx,"mean":mu,"std":sigma}

numeric_samplers = {}
numeric_meta = {}
for c in numeric_cols:
    info = schema[c]
    sampler, meta = make_numeric_sampler(info)
    numeric_samplers[c] = sampler
    numeric_meta[c] = meta

# -------------------------
# Infer soft correlation groups
# We'll group by keywords: size = {radius, area, perimeter}, shape = {compactness, concavity, concave points}, texture = {texture}
# Within a group: high correlation (0.7). Between groups: moderate (0.25). User rules can adjust weights.
# -------------------------
groups = {
    "size": [],
    "shape": [],
    "texture": [],
    "other": []
}
for c in numeric_cols:
    lc = c.lower()
    if any(k in lc for k in ["radius","area","perimeter"]):
        groups["size"].append(c)
    elif any(k in lc for k in ["concav","concave","compact","concavity"]):
        groups["shape"].append(c)
    elif "texture" in lc:
        groups["texture"].append(c)
    else:
        groups["other"].append(c)

# build correlation matrix
n = len(numeric_cols)
if n > 0:
    idx = {c:i for i,c in enumerate(numeric_cols)}
    corr = np.eye(n)
    # intra-group strong correlation
    for gname,gcols in groups.items():
        for i in range(len(gcols)):
            for j in range(i+1,len(gcols)):
                ci = idx[gcols[i]]; cj = idx[gcols[j]]
                corr[ci,cj] = corr[cj,ci] = 0.7
    # inter-group moderate correlation
    for g1 in groups:
        for g2 in groups:
            if g1 == g2: continue
            for c1 in groups[g1]:
                for c2 in groups[g2]:
                    i1 = idx[c1]; i2 = idx[c2]
                    if corr[i1,i2] == 1.0:
                        continue
                    corr[i1,i2] = corr[i2,i1] = 0.25
    # ensure PD by jitter if necessary
    def make_pd(mat):
        jitter = 1e-6
        tries = 0
        while True:
            try:
                np.linalg.cholesky(mat)
                return mat
            except np.linalg.LinAlgError:
                mat = mat + np.eye(mat.shape[0])*jitter
                jitter *= 10
                tries += 1
                if tries>10:
                    return mat
    corr = make_pd(corr)
else:
    corr = None

# -------------------------
# Generate correlated numeric samples via Gaussian copula:
# 1) sample multivariate normal N(0, corr)
# 2) map standard normals to uniform via phi
# 3) map uniform to each feature's marginal using inverse CDF approximated by sampling from the marginal sampler
# We'll pre-sample a large grid for inversion per feature for speed.
# -------------------------
def build_inverse_lookup(sampler, meta, n_grid=5000):
    grid = sampler(n_grid)
    grid_sorted = np.sort(grid)
    u = (np.arange(n_grid) + 0.5) / n_grid
    return grid_sorted, u

inverse_lookup = {}
for c in numeric_cols:
    grid, u = build_inverse_lookup(numeric_samplers[c], numeric_meta[c], n_grid=5000)
    inverse_lookup[c] = (grid, u)

from scipy.stats import norm as _norm

def invert_uniform_to_marginal(u_vals, grid, u_grid):
    # u_vals in (0,1) map via linear interpolation on u_grid -> grid
    return np.interp(u_vals, u_grid, grid)

def generate_numeric_block(n_rows):
    if len(numeric_cols) == 0:
        return pd.DataFrame(index=range(n_rows))
    # sample multivar normal
    L = np.linalg.cholesky(corr)
    z = np.random.normal(size=(n_rows, len(numeric_cols)))
    mvn = z.dot(L.T)  # correlated normals
    u_vals = _norm.cdf(mvn)  # uniform [0,1]
    out = {}
    for j,c in enumerate(numeric_cols):
        grid, u_grid = inverse_lookup[c]
        out[c] = invert_uniform_to_marginal(u_vals[:,j], grid, u_grid)
    return pd.DataFrame(out)

# -------------------------
# Categorical handling (diagnosis, etc.)
# If schema contains categorical with top_values, we implement:
# - default prior from uniform or top_values frequency if provided (schema may optionally provide 'freqs')
# - apply rules: if schema.rules includes threshold rules they are applied/boosted
# - major categorical 'diagnosis' will be sampled conditionally from numeric size_score via logistic-like function
# -------------------------
def compute_size_score(df_row):
    # heuristic: combine normalized radius/area/perimeter relative to their means
    score = 0.0
    count = 0
    for c in numeric_cols:
        lc = c.lower()
        if any(k in lc for k in ["radius","area","perimeter"]):
            meta = numeric_meta[c]
            mean = meta["mean"]
            std = meta["std"] if meta["std"]>0 else 1.0
            score += (df_row[c] - mean) / std
            count += 1
    return score / max(1, count)

def sample_categorical_column(col, df_numeric_row):
    info = schema[col]
    top = info.get("top_values", None)
    freqs = info.get("freqs", None)
    # special handling if column is 'diagnosis' or user provided rules exist
    # base_prob_M from size_score (sigmoid)
    if col.lower() == "diagnosis":
        size_score = compute_size_score(df_numeric_row)
        # convert size_score to probability using sigmoid
        p_M = 1 / (1 + math.exp(-0.6*(size_score - 0.5)))  # tuning constants
        # Apply any schema rule overrides e.g. rules: {"diagnosis_from_area": {"if_area_gt":700,"force_prob_M":0.9}}
        for rname, rinfo in schema_rules.items():
            # simple pattern matching in rules
            if "if_area_gt" in rinfo and df_numeric_row.get("area_mean", 0) > rinfo["if_area_gt"]:
                p_M = max(p_M, rinfo.get("force_prob_M", p_M))
            if "if_radius_gt" in rinfo and df_numeric_row.get("radius_mean", 0) > rinfo["if_radius_gt"]:
                p_M = max(p_M, rinfo.get("force_prob_M", p_M))
        # ensure top values contains "M" and "B"
        classes = top if top else ["B","M"]
        # sample based on p_M
        return "M" if np.random.rand() < p_M else "B"
    else:
        # general categorical sampling
        if freqs and top and len(freqs)==len(top):
            return np.random.choice(top, p=freqs)
        elif top:
            return np.random.choice(top)
        else:
            # fallback generic two classes
            return str(np.random.choice(["A","B"]))

# -------------------------
# String/date sampling: use Faker fallback or schema 'top_values'
# -------------------------
def sample_string(col):
    info = schema.get(col, {})
    tops = info.get("top_values", None)
    if tops:
        return np.random.choice(tops)
    lc = col.lower()
    if "name" in lc:
        return fake.first_name()
    if "city" in lc:
        return fake.city()
    return fake.word()

# -------------------------
# Main generator
# -------------------------
def generate_synthetic(n_rows=100000, explain_sample_index=None):
    # numeric
    numeric_df = generate_numeric_block(n_rows)
    # categorical
    for cat in categorical_cols:
        vals = []
        for i in range(n_rows):
            rownum = numeric_df.iloc[i]
            vals.append(sample_categorical_column(cat, rownum))
        numeric_df[cat] = vals
    # strings/dates
    for s in string_cols:
        numeric_df[s] = [sample_string(s) for _ in range(n_rows)]
    for d in date_cols:
        # random recent dates within a window using schema min/max if present
        info = schema.get(d, {})
        # fallback sample: generate last 5 years random
        import datetime, random
        now = datetime.datetime.now()
        def rand_date():
            days = random.randint(0, 5*365)
            return (now - datetime.timedelta(days=days)).date().isoformat()
        numeric_df[d] = [rand_date() for _ in range(n_rows)]
    # return df
    return numeric_df

# -------------------------
# Run generation (100k)
# -------------------------
NUM_ROWS = 100000
print(f"Generating {NUM_ROWS:,} rows ... (this may take 20-60 seconds depending on environment)")
synthetic = generate_synthetic(NUM_ROWS)
out_path = "/content/synthetic_100k.csv"
synthetic.to_csv(out_path, index=False)
print("Saved:", out_path)
if _IN_COLAB:
    colab_files.download(out_path)

# -------------------------
# Explanation helper
# -------------------------
def explain_row(row, target='diagnosis', top_k=6):
    if target not in row.index:
        return f"Target column {target} not in dataset."
    explanation_lines = []
    pred = row[target]
    explanation_lines.append(f"Predicted {target} = {pred}.")
    # compute size score and show key contributors
    size_feats = [c for c in numeric_cols if any(k in c.lower() for k in ['radius','area','perimeter'])]
    contributions = []
    for f in size_feats:
        if f in row:
            meta = numeric_meta.get(f, {})
            med = meta.get("mean", None)
            val = row[f]
            if med is None:
                rel = "no median provided"
            else:
                rel = "higher than" if val > med else ("lower than" if val < med else "about equal to")
            contributions.append((f, val, med, rel))
    # sort by absolute difference from mean
    contributions.sort(key=lambda x: abs((x[1] - (x[2] if x[2] is not None else 0))), reverse=True)
    explanation_lines.append("Top size-related contributions:")
    for f,val,med,rel in contributions[:top_k]:
        explanation_lines.append(f"- {f}: {val:.3f} ({rel} median {med:.3f})")
    # Add rule-based reasons from schema.rules if any matched
    if schema_rules:
        matched = []
        for rname,rinfo in schema_rules.items():
            if "if_area_gt" in rinfo and row.get("area_mean",0) > rinfo["if_area_gt"]:
                matched.append(f"Rule {rname}: area_mean {row.get('area_mean'):.1f} > {rinfo['if_area_gt']}")
            if "if_radius_gt" in rinfo and row.get("radius_mean",0) > rinfo["if_radius_gt"]:
                matched.append(f"Rule {rname}: radius_mean {row.get('radius_mean'):.1f} > {rinfo['if_radius_gt']}")
        if matched:
            explanation_lines.append("\nMatched schema rules influencing diagnosis:")
            explanation_lines += ["- "+m for m in matched]
    # Short final sentence matching your requested format
    explanation_lines.append("\nFinal statement:")
    # Example: "Based on radius_mean=14.657 and area_worst=880.327 I have given diagnosis = M"
    # pick top two numeric contributors
    top_feats = contributions[:2]
    if top_feats:
        parts = [f"{f}={val:.3f}" for f,val,_,_ in top_feats]
        explanation_lines.append(f"Based on {', '.join(parts)} I have given diagnosis = {pred}.")
    else:
        explanation_lines.append(f"Diagnosis = {pred} assigned by probabilistic model + schema rules.")
    return "\n".join(explanation_lines)

# -------------------------
# Example: explain row 0
# -------------------------
print("\n--- Example explanation for generated row 0 ---")
print(explain_row(synthetic.iloc[0], target=(categorical_cols[0] if categorical_cols else None)))

"""### **FINAL ONE**"""

# Final Colab-ready: Synthetic Dataset Generator (Option C, with default medical correlations)
# Saves: /content/synthetic_dataset.csv (100,000 rows)
# Explanation helper included: explain_row(row, target='diagnosis')
# Usage in Colab: upload schema.json when prompted

# -------------------------
# Install required libs
# -------------------------
!pip install faker tqdm scipy -q

# -------------------------
# Imports
# -------------------------
import os, json, math, random
import numpy as np
import pandas as pd
from faker import Faker
from tqdm import tqdm
from scipy.stats import truncnorm, norm
try:
    from google.colab import files as colab_files
    _IN_COLAB = True
except Exception:
    _IN_COLAB = False

fake = Faker()
np.random.seed(42)
random.seed(42)

# -------------------------
# Load schema.json
# -------------------------
if _IN_COLAB:
    print("Please upload your schema.json file via the browser dialog.")
    uploaded = colab_files.upload()
    schema_file = None
    for k in uploaded.keys():
        if k.lower().endswith(".json"):
            schema_file = k
else:
    schema_file = "schema.json"
    if not os.path.exists(schema_file):
        raise FileNotFoundError("Put schema.json in working directory or run in Colab and upload.")

if schema_file is None:
    raise ValueError("No schema.json provided.")

with open(schema_file, "r") as f:
    schema = json.load(f)

# -------------------------
# Detect columns by type (from schema)
# -------------------------
numeric_cols = []
categorical_cols = []
string_cols = []
date_cols = []
schema_rules = schema.get("rules", {})

for col, info in schema.items():
    if col == "rules":
        continue
    t = str(info.get("type","")).lower()
    if t in ("numeric","number","float","double","int","integer"):
        numeric_cols.append(col)
    elif t in ("categorical","category"):
        categorical_cols.append(col)
    elif t in ("string","text"):
        string_cols.append(col)
    elif t in ("date","datetime"):
        date_cols.append(col)
    else:
        # default guess
        numeric_cols.append(col)

print("Columns detected:")
print(" Numeric:", numeric_cols)
print(" Categorical:", categorical_cols)
print(" String:", string_cols)
print(" Date:", date_cols)

# -------------------------
# Helpers: build numeric samplers (truncated normal) using schema stats
# -------------------------
def make_sampler_from_info(info):
    # return function samp(n)
    mn = info.get("min", None)
    mx = info.get("max", None)
    mu = info.get("mean", None)
    sigma = info.get("std", None)

    # infer mean/std if missing
    if mu is None and mn is not None and mx is not None:
        mu = 0.5*(mn+mx)
    if sigma is None:
        if mn is not None and mx is not None:
            sigma = max(1e-3, (mx-mn)/6.0)
        else:
            sigma = 1.0

    if mn is None: mn = mu - 4*sigma if mu is not None else -4*sigma
    if mx is None: mx = mu + 4*sigma if mu is not None else 4*sigma

    # avoid degenerate sigma=0
    if sigma <= 0 or mu is None:
        def sampler(n):
            return np.full(n, mu if mu is not None else 0.0)
        meta = {"min": mn, "max": mx, "mean": mu, "std": sigma}
        return sampler, meta

    a = (mn - mu) / sigma
    b = (mx - mu) / sigma

    def sampler(n):
        return truncnorm.rvs(a, b, loc=mu, scale=sigma, size=n)

    meta = {"min": mn, "max": mx, "mean": mu, "std": sigma}
    return sampler, meta

numeric_samplers = {}
numeric_meta = {}
for c in numeric_cols:
    info = schema.get(c, {})
    sampler, meta = make_sampler_from_info(info)
    numeric_samplers[c] = sampler
    numeric_meta[c] = meta

# -------------------------
# Domain-aware correlation setup (medical defaults)
# - size group: radius*, area*, perimeter* -> strongly correlated
# - shape group: concavity, concave_points, compactness -> correlated with size moderately
# - texture group -> moderate correlation with size
# We'll produce a correlation matrix for the numeric columns.
# -------------------------
groups = {"size": [], "shape": [], "texture": [], "other": []}
for c in numeric_cols:
    lc = c.lower()
    if any(k in lc for k in ["radius", "area", "perimeter"]):
        groups["size"].append(c)
    elif any(k in lc for k in ["concav", "concave", "compact", "concavity"]):
        groups["shape"].append(c)
    elif "texture" in lc:
        groups["texture"].append(c)
    else:
        groups["other"].append(c)

idx = {c:i for i,c in enumerate(numeric_cols)}
n = len(numeric_cols)
if n > 0:
    corr = np.eye(n)
    # intra-group strong (0.7-0.9 depend)
    for gcols in groups.values():
        for i in range(len(gcols)):
            for j in range(i+1, len(gcols)):
                ci = idx[gcols[i]]; cj = idx[gcols[j]]
                corr[ci,cj] = corr[cj,ci] = 0.85 if gcols is groups["size"] else 0.65
    # inter-group moderate
    for g1 in groups:
        for g2 in groups:
            if g1 == g2: continue
            for c1 in groups[g1]:
                for c2 in groups[g2]:
                    i1 = idx[c1]; i2 = idx[c2]
                    if corr[i1,i2] == 1.0:
                        continue
                    corr[i1,i2] = corr[i2,i1] = 0.25
    # ensure positive-definite by jitter
    def make_pd(m):
        jitter = 1e-6
        tries = 0
        while True:
            try:
                np.linalg.cholesky(m)
                return m
            except np.linalg.LinAlgError:
                m = m + np.eye(m.shape[0])*jitter
                jitter *= 10
                tries += 1
                if tries > 10:
                    return m
    corr = make_pd(corr)
else:
    corr = None

# -------------------------
# Pre-build inverse-lookups for marginal inversion
# For each numeric feature pre-sample a grid to do inverse-mapping from uniform(0,1) quantiles
# -------------------------
inverse_lookup = {}
GRID_SIZE = 4000
for c in numeric_cols:
    grid = numeric_samplers[c](GRID_SIZE)
    grid_sorted = np.sort(grid)
    u_grid = (np.arange(GRID_SIZE) + 0.5) / GRID_SIZE
    inverse_lookup[c] = (grid_sorted, u_grid)

# -------------------------
# Helper: generate correlated numeric block via Gaussian copula
# -------------------------
from scipy.stats import norm as _norm
def generate_numeric_block(n_rows):
    if n == 0:
        return pd.DataFrame(index=range(n_rows))
    L = np.linalg.cholesky(corr)
    z = np.random.normal(size=(n_rows, n))
    mvn = z.dot(L.T)
    u = _norm.cdf(mvn)  # uniforms
    out = {}
    for j,c in enumerate(numeric_cols):
        grid, u_grid = inverse_lookup[c]
        out[c] = np.interp(u[:,j], u_grid, grid)
    return pd.DataFrame(out)

# -------------------------
# Domain-based deterministic-ish adjustments to enforce physics-like relations:
# We'll nudge area and perimeter to approximate relationships with radius when those columns exist.
# area â‰ˆ k * pi * radius^2  (k ~ 0.9-1.3 to allow noise)
# perimeter â‰ˆ k2 * 2*pi*radius (k2 ~ 0.85-1.2)
# If radius not present but area & perimeter present, keep as sampled.
# We apply a mild convex combination (alpha) toward the formula to preserve marginal stats.
# -------------------------
def apply_medical_relations(df, alpha=0.35):
    # alpha: how strongly to force formula (0 = don't change, 1 = fully enforce)
    df = df.copy()
    # if radius_mean present, adjust area_mean / perimeter_mean / area_worst / perimeter_worst using radius variants
    # attempt multiple radius variants if exist
    rad_keys = [k for k in numeric_cols if "radius" in k.lower()]
    area_keys = [k for k in numeric_cols if "area" in k.lower()]
    perim_keys = [k for k in numeric_cols if "perimeter" in k.lower()]
    # pick canonical radius_mean if present else radius_worst else any radius
    rad_primary = None
    for candidate in ["radius_mean","radius_worst","radius_se","radius"]:
        if candidate in df.columns:
            rad_primary = candidate
            break
    if rad_primary is None and len(rad_keys)>0:
        rad_primary = rad_keys[0]
    # apply adjustments row-wise (vectorized)
    if rad_primary is not None:
        r = df[rad_primary].values.astype(float)
        # area formula
        for akey in area_keys:
            # base sampled area
            base = df[akey].values.astype(float)
            # compute formula-based area (k * pi * r^2) with random k per-row ~ Uniform(0.9,1.2)
            k = np.random.uniform(0.9,1.2, size=len(r))
            formula = k * math.pi * (r**2)
            # new = (1-alpha)*base + alpha*formula
            df[akey] = ((1-alpha)*base + alpha*formula)
        # perimeter formula
        for pkey in perim_keys:
            base = df[pkey].values.astype(float)
            k2 = np.random.uniform(0.85,1.15, size=len(r))
            formula_p = k2 * 2 * math.pi * r
            df[pkey] = ((1-alpha)*base + alpha*formula_p)
    # small clamp to metadata bounds (if schema gives min/max)
    for c in numeric_cols:
        meta = numeric_meta.get(c, {})
        mn = meta.get("min", None); mx = meta.get("max", None)
        if mn is not None:
            df[c] = np.maximum(df[c], mn)
        if mx is not None:
            df[c] = np.minimum(df[c], mx)
    return df

# -------------------------
# Categorical sampling (diagnosis special case)
# - diagnosis probability driven by a size_score computed from size features
# - also honor any schema.rules that may raise probability
# -------------------------
def compute_size_score_row(row):
    score = 0.0
    count = 0
    for c in numeric_cols:
        if any(k in c.lower() for k in ["radius","area","perimeter"]):
            meta = numeric_meta.get(c, {})
            mean = meta.get("mean", 1.0)
            std = meta.get("std", 1.0) if meta.get("std", 1.0)>0 else 1.0
            score += (float(row[c]) - mean) / std
            count += 1
    return (score / max(1, count)) if count>0 else 0.0

def sample_diagnosis(row):
    # sigmoid mapping from size_score to probability of M
    size_score = compute_size_score_row(row)
    # constants tuned for moderate sensitivity
    p_M = 1.0 / (1.0 + math.exp(-0.55*(size_score - 0.6)))
    # apply schema.rules overrides if any
    for rname, rinfo in schema_rules.items():
        # support simple rule shapes: if_area_gt, if_radius_gt with force_prob_M
        if isinstance(rinfo, dict):
            if "if_area_gt" in rinfo and row.get("area_mean", 0) > rinfo["if_area_gt"]:
                p_M = max(p_M, rinfo.get("force_prob_M", p_M))
            if "if_radius_gt" in rinfo and row.get("radius_mean", 0) > rinfo["if_radius_gt"]:
                p_M = max(p_M, rinfo.get("force_prob_M", p_M))
    # sample
    return "M" if np.random.rand() < p_M else "B", p_M

# generic categorical sampler fallback
def sample_categorical_generic(col):
    info = schema.get(col, {})
    tops = info.get("top_values", None)
    freqs = info.get("freqs", None)
    if freqs and tops and len(freqs) == len(tops):
        return np.random.choice(tops, p=freqs)
    elif tops:
        return np.random.choice(tops)
    else:
        return random.choice(["A","B","C"])

# -------------------------
# String / date samplers
# -------------------------
def sample_string(col):
    info = schema.get(col, {})
    tops = info.get("top_values", None)
    if tops:
        return random.choice(tops)
    lc = col.lower()
    if "name" in lc:
        return fake.first_name()
    if "city" in lc or "town" in lc:
        return fake.city()
    return fake.word()

def sample_date(col):
    # if schema has min/max dates use them; fallback to last 5 years
    info = schema.get(col, {})
    mn = info.get("min", None)
    mx = info.get("max", None)
    import datetime
    if mn and mx:
        try:
            mn_dt = pd.to_datetime(mn)
            mx_dt = pd.to_datetime(mx)
            low = int(mn_dt.value // 10**9)
            high = int(mx_dt.value // 10**9)
            ts = random.randint(low, high)
            return pd.to_datetime(ts, unit='s').date().isoformat()
        except Exception:
            pass
    now = pd.Timestamp.now()
    days = random.randint(0, 5*365)
    return (now - pd.Timedelta(days=days)).date().isoformat()

# -------------------------
# Main generator function
# -------------------------
def generate_synthetic(n_rows=100000, enforce_rel_alpha=0.35):
    # 1) numeric correlated block
    numeric_df = generate_numeric_block(n_rows)
    # 2) apply domain nudging (formulas)
    numeric_df = apply_medical_relations(numeric_df, alpha=enforce_rel_alpha)
    # 3) categorical columns
    for cat in categorical_cols:
        vals = []
        probs = []
        for i in range(n_rows):
            row = numeric_df.iloc[i]
            if cat.lower() == "diagnosis":
                v, p = sample_diagnosis(row)
                vals.append(v); probs.append(p)
            else:
                vals.append(sample_categorical_generic(cat))
        numeric_df[cat] = vals
        # optionally store the probabilities as well (not requested)
        if cat.lower() == "diagnosis":
            numeric_df["_prob_M"] = probs
    # 4) strings & dates
    for s in string_cols:
        numeric_df[s] = [sample_string(s) for _ in range(n_rows)]
    for d in date_cols:
        numeric_df[d] = [sample_date(d) for _ in range(n_rows)]
    # 5) reorder columns to exactly follow schema.json order
    final_cols = [c for c in schema.keys() if c != "rules"]
    # If any generated column missing in final_cols (rare), append it
    for c in numeric_df.columns:
        if c not in final_cols:
            final_cols.append(c)
    df_out = numeric_df[final_cols]
    return df_out

# -------------------------
# Run generation and save CSV
# -------------------------
NUM_ROWS = 1000000
print(f"Generating {NUM_ROWS:,} rows. This may take 20-60 seconds...")
synthetic_df = generate_synthetic(NUM_ROWS, enforce_rel_alpha=0.35)
out_path = "/content/synthetic_dataset1.csv"
synthetic_df.to_csv(out_path, index=False)
print("Saved synthetic dataset to:", out_path)
if _IN_COLAB:
    colab_files.download(out_path)

# -------------------------
# Explanation helper (your requested sentence format)
# -------------------------
def explain_row(row, target='diagnosis', top_k=3):
    """
    Returns a human readable explanation, ending with:
    'Based on {col1=value1}, {col2=value2} I have given diagnosis = M' (or B)
    """
    if target not in row.index:
        return f"Target {target} not present."
    pred = row[target]
    explanation = []
    explanation.append(f"Diagnosis assigned using a size-driven probabilistic model (size features => higher chance of 'M').")
    # compute size-related contributions
    size_feats = [c for c in numeric_cols if any(k in c.lower() for k in ["radius","area","perimeter"])]
    contributions = []
    for f in size_feats:
        if f in row.index:
            meta = numeric_meta.get(f, {})
            mean = meta.get("mean", None)
            val = float(row[f])
            if mean is not None:
                rel = "higher than" if val > mean else ("lower than" if val < mean else "about equal to")
            else:
                rel = "no mean provided"
            diff = abs(val - (mean if mean is not None else 0))
            contributions.append((f, val, mean, rel, diff))
    # sort by difference from mean
    contributions.sort(key=lambda x: x[4], reverse=True)
    explanation.append("Top contributors (feature: value (comparison to schema mean)):")
    for f,val,mean,rel,_ in contributions[:top_k]:
        explanation.append(f"- {f}: {val:.3f} ({rel} mean {mean:.3f})" if mean is not None else f"- {f}: {val:.3f}")
    # include any rule matches
    matched_rules = []
    for rname, rinfo in schema_rules.items():
        if isinstance(rinfo, dict):
            if "if_area_gt" in rinfo and row.get("area_mean",0) > rinfo["if_area_gt"]:
                matched_rules.append(f"{rname}: area_mean {row.get('area_mean'):.1f} > {rinfo['if_area_gt']}")
            if "if_radius_gt" in rinfo and row.get("radius_mean",0) > rinfo["if_radius_gt"]:
                matched_rules.append(f"{rname}: radius_mean {row.get('radius_mean'):.1f} > {rinfo['if_radius_gt']}")
    if matched_rules:
        explanation.append("\nMatched schema rules influencing diagnosis:")
        for m in matched_rules:
            explanation.append(f"- {m}")
    # final requested sentence
    top_two = contributions[:2]
    if top_two:
        parts = [f"{f}={val:.3f}" for f,val,_,_,_ in top_two]
        explanation.append(f"\nFinal statement: Based on {', '.join(parts)} I have given diagnosis = {pred}.")
    else:
        explanation.append(f"\nFinal statement: Diagnosis = {pred} assigned by probabilistic model and schema rules.")
    return "\n".join(explanation)

# -------------------------
# Example: show first generated row and explanation
# -------------------------
print("\n--- Sample generated row (index 0) ---")
print(synthetic_df.iloc[0].to_dict())
print("\n--- Explanation for row 0 ---")
print(explain_row(synthetic_df.iloc[0], target=(categorical_cols[0] if categorical_cols else 'diagnosis')))

"""**PART BY PART**"""

import json
import numpy as np
import pandas as pd
from scipy.stats import truncnorm

# Load schema
schema_path = "metadata.json"   # change only if needed
with open(schema_path, "r") as f:
    schema = json.load(f)

print("Schema loaded with", len(schema), "columns")

def truncated_normal(mean, std, min_val, max_val, size=1):
    if std == 0:
        return np.full(size, mean)
    a, b = (min_val - mean) / std, (max_val - mean) / std
    return truncnorm.rvs(a, b, loc=mean, scale=std, size=size)
numeric_cols = []
categorical_cols = []

for col, meta in schema.items():
    if meta["type"] in ["int", "float", "numeric"]:
        numeric_cols.append(col)
    elif meta["type"] == "categorical":
        categorical_cols.append(col)

print("Numeric:", len(numeric_cols), "Categorical:", len(categorical_cols))

def generate_single_row(schema):
    row = {}
    explanation = {}

    # ---- Step 1: numeric columns ----
    for col in numeric_cols:
        meta = schema[col]
        min_v = meta["min"]
        max_v = meta["max"]
        mean = meta.get("mean", (min_v + max_v) / 2)
        std = meta.get("std", (max_v - min_v) * 0.2)  # <-- FIXED

        value = float(truncated_normal(mean, std, min_v, max_v, 1)[0])
        row[col] = round(value, 3)

        explanation[col] = {
            "distribution": "truncated_normal",
            "mean_used": mean,
            "std_used": std,
            "min": min_v,
            "max": max_v,
            "reason": f"{col} generated using automatic std={std:.3f}"
        }

    # ---- Step 2: categorical columns ----
    for col in categorical_cols:
        meta = schema[col]
        values = meta["top_values"]

        # special logic for diagnosis
        if col.lower() == "diagnosis":
            radius = row.get("radius_mean", 0)
            area = row.get("area_mean", 0)

            if radius > 14 and area > 550:
                chosen = "M"
                logic = f"High radius_mean ({radius}) & area_mean ({area}) â†’ malignant"
            else:
                chosen = "B"
                logic = f"Lower tumour geometry radius_mean ({radius}) & area_mean ({area}) â†’ benign"

            row[col] = chosen
            explanation[col] = {
                "diagnosis_logic": logic
            }
            continue

        # normal categorical
        chosen = np.random.choice(values)
        row[col] = chosen
        explanation[col] = {
            "reason": f"Random categorical choice from {values}"
        }

    return row, explanation

def build_row_explanation(row):
    exp = {}

    for col, val in row.items():

        # add existing explanation if present
        exp[col] = {}

        # correlation logic examples
        if "radius" in col:
            exp[col]["correlation"] = "Radius correlates positively with area and perimeter"

        if "area" in col:
            exp[col]["derived_from"] = "area â‰ˆ Ï€ Ã— (radius_meanÂ²) with noise"

        if "concavity" in col:
            exp[col]["relationship"] = "Concavity increases with tumour aggressiveness"

        # final numeric/categorical info added later
        # handled by generate_single_row()
    return exp

rows = []
explanations = []

for i in range(2):  # only 2 detailed rows
    r, e = generate_single_row(schema)
    rows.append(r)

    full_exp = build_row_explanation(r)

    # merge both explanation parts
    for key in e:
        full_exp[key] = {**full_exp.get(key, {}), **e[key]}

    explanations.append(full_exp)

# Save explanations to file
with open("explanations.txt", "w") as f:
    for i, exp in enumerate(explanations):
        f.write(f"==================== Row {i+1} ====================\n")
        for col, details in exp.items():
            f.write(f"\n{col}: {rows[i][col]}\n")
            for k, v in details.items():
                f.write(f"  - {k}: {v}\n")
        f.write("\n\n")

print("Saved explanations â†’ explanations.txt")

# ===============================
# Part 7 â€” Save & Download Files
# ===============================

import os
from google.colab import files
import json
import pandas as pd

# --- Function to generate unique filenames ---
def unique_name(base, ext):
    counter = 1
    while True:
        fname = f"{base}_{counter}.{ext}"
        if not os.path.exists(fname):
            return fname
        counter += 1

# --- Create unique filenames ---
csv_name = unique_name("synthetic_dataset", "csv")
json_name = unique_name("explanations", "json")

# --- Save dataset ---
synthetic_df.to_csv(csv_name, index=False)

# --- Save explanations ---
with open(json_name, "w") as f:
    json.dump(explanations, f, indent=4)

print(f"Dataset saved as: {csv_name}")
print(f"Explanations saved as: {json_name}")

# --- Auto-download in Colab ---
files.download(csv_name)
files.download(json_name)